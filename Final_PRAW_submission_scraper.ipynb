{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "import random\n",
    "\n",
    "#testing autocomplete\n",
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 5.3.0 of praw is outdated. Version 5.4.0 was released Wednesday March 28, 2018.\n"
     ]
    }
   ],
   "source": [
    "client_id= 'UTeG611HjpoNLg' #'zB8oNVtsNw-A2A'\n",
    "client_secret= \t'oVIpII6MhJb8Rsmtm1mvmhBVyIg' #'U0fn882ZjwhZhR9pejWthakz3iA'\n",
    "reddit_user_agent='test code for /u/ismn5960_scraper'\n",
    "\n",
    "reddit = praw.Reddit(client_id= client_id,\n",
    "                     client_secret= client_secret,\n",
    "                     user_agent='app for /u/ISMN5960_scraper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Previously, this function was made to take a start date, an end date, and a subreddit and then pull extended data for submissions to that subreddit between the start and end date. This was done using the PRAW Python package. \n",
    "\n",
    "##### However around _____ day Reddit stopping using the search service that allowed to me to pull submission info from a set of dates. This required me to adjust my data collection method. \n",
    "\n",
    "##### Now this function is made to take file with a list of submission ids and used the PRAW Python wrapper to get extended data for each of those submissions and then put it in a separate Json file for analysis. Submission ids are gather using a third party service called Pushift.io. The methods used for gathering the submission ids can be found here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submissionInfoScraper(file_with_ids):\n",
    "    \n",
    "    file = open(file_with_ids , \"rb\")\n",
    "    jsonObject = json.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    start_timer = timer()\n",
    "    current_date = datetime.date.today()\n",
    "    submission_count = 0\n",
    "    subreddit_submission_info = []\n",
    "    fields = ('title', 'author', 'subreddit','selftext', 'num_comments', 'score', 'upvote_ratio', 'created_utc', 'gilded', 'url', 'over_18')\n",
    "    \n",
    "    for entry in jsonObject['id']:\n",
    "        submission = reddit.submission(id=entry)\n",
    "        turn_into_non_lazy_object = submission.title\n",
    "        to_dict = (vars(submission))\n",
    "        individual_submission_dict = {field:str(to_dict[field]) for field in fields}\n",
    "        individual_submission_dict['ups'] = round(float(individual_submission_dict['score']) / float(individual_submission_dict['upvote_ratio']))\n",
    "        individual_submission_dict['downs'] = (individual_submission_dict['ups'] - float(individual_submission_dict['score']))    \n",
    "        subreddit_submission_info.append(individual_submission_dict)\n",
    "        submission_count += 1\n",
    "        \n",
    "    with open(file.name.split('-')[0] + '-' + str(current_date) + '-submissionData.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(subreddit_submission_info, file)\n",
    "        file.close()\n",
    "    \n",
    "    print('/r/' + file.name.split('-')[0], 'submission count: ' + str(submission_count))\n",
    "    print('File saved as: ' + file.name.split('-')[0] + '-' + str(current_date) + '-subMissiondata.json')\n",
    "    print('Program took ' + str(timer() - start_timer) + ' seconds to run') #in seconds  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/r/the_Donald submission count: 188795\n",
      "File saved as: the_Donald-2018-04-27-subMissiondata.json\n",
      "Program took 27394.644117731626 seconds to run\n"
     ]
    }
   ],
   "source": [
    "submissionInfoScraper('the_Donald-submission-ids.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However for this project, I had to cut down on the amount of submissions I gathered so that it wouldn't take as long to gather the data. The below code was used to get the full data on 1/10th of the total submissions I had ids for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('the_Donald-submission-ids.json' , \"rb\")\n",
    "jsonObject = json.load(file)\n",
    "file.close()\n",
    "\n",
    "test = list(jsonObject['id'])\n",
    "\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188795.6\n"
     ]
    }
   ],
   "source": [
    "print(len(test)/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_timer = timer()\n",
    "current_date = datetime.date.today()\n",
    "submission_count = 0\n",
    "subreddit_submission_info = []\n",
    "fields = ('title', 'author', 'subreddit','selftext', 'num_comments', 'score', 'upvote_ratio', 'created_utc', 'gilded', 'url', 'over_18')\n",
    "    \n",
    "for entry in test[:188795]:\n",
    "    submission = reddit.submission(id=entry)\n",
    "    turn_into_non_lazy_object = submission.title\n",
    "    to_dict = (vars(submission))\n",
    "    individual_submission_dict = {field:str(to_dict[field]) for field in fields}\n",
    "    individual_submission_dict['ups'] = round(float(individual_submission_dict['score']) / float(individual_submission_dict['upvote_ratio']))\n",
    "    individual_submission_dict['downs'] = (individual_submission_dict['ups'] - float(individual_submission_dict['score']))    \n",
    "    subreddit_submission_info.append(individual_submission_dict)\n",
    "    submission_count += 1\n",
    "        \n",
    "with open(file.name.split('-')[0] + '-' + str(current_date) + '-submissionData.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(subreddit_submission_info, file)\n",
    "    file.close()\n",
    "    \n",
    "print('/r/' + file.name.split('-')[0], 'submission count: ' + str(submission_count))\n",
    "print('File saved as: ' + file.name.split('-')[0] + '-' + str(current_date) + '-subMissiondata.json')\n",
    "print('Program took ' + str(timer() - start_timer) + ' seconds to run') #in seconds  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
